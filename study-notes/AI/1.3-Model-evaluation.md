# Evaluating Foundation Models

### **Automatic Evaluation**
- Built-in task types: 
  - **Text Summarization**, **Q&A**, **Text Classification**, **Open-Ended Text Generation**.
- Use **Benchmark Datasets**:
  - AWS-curated or custom datasets.
  - Consist of **benchmark questions** and **ideal benchmark answers**.
- Grading via a **Judge Model**:
  - Compares generated answers with benchmark answers.
  - Scores calculated using metrics like:
    - **ROUGE**: Measures overlap in text (useful for summarization).
    - **BLEU**: Focuses on translation precision, penalizes brevity.
    - **BERTScore**: Measures semantic similarity using embeddings.
    - **Perplexity**: Evaluates token prediction accuracy (lower is better).

### **Human Evaluation**
- Humans (e.g., SMEs, employees) evaluate the model's responses.
- Metrics include thumbs-up/down, ranking, etc.
- Allows more freedom with **custom tasks**.

---

## **3. Evaluation Metrics Overview**
| **Metric**     | **Purpose**                              | **Key Features**                          |
|-----------------|------------------------------------------|-------------------------------------------|
| **ROUGE**       | Summarization, machine translation       | Measures n-gram matches and longest subsequence overlap. |
| **BLEU**        | Translation tasks                       | Considers precision, penalizes brevity.   |
| **BERTScore**   | Semantic similarity                     | Compares embeddings for nuanced meanings. |
| **Perplexity**  | Predictive confidence                   | Lower values indicate better predictions. |

---

## **4. Business Metrics**
- **User Satisfaction**: Feedback on response quality.
- **Conversion Rates**: Measure success in achieving outcomes (e.g., higher sales).
- **Cross-Domain Performance**: Ability to handle varied tasks across domains.
- **Efficiency**: Measures computational cost and resource utilization.

---

## **5. Transfer Learning**
- Broader concept than fine-tuning.
- Adapts a pre-trained model for a related but different task.
- Example: Using BERT for domain-specific text analysis.

---

## **Exam Tips**
1. **Understand Key Metrics**:
   - ROUGE: Summarization tasks.
   - BLEU: Translation tasks.
   - BERTScore: Context and semantic similarity.
   - Perplexity: Predictive accuracy.
2. **Benchmark Dataset Knowledge**:
   - AWS-curated datasets cover common scenarios.
   - Custom datasets are essential for business-specific tasks.
3. **Recognize Evaluation Methods**:
   - Automatic: Judge model comparison.
   - Human: Subject matter experts evaluating flexibility and nuance.
4. **Business Metrics Application**:
   - Expect questions on user satisfaction and cost efficiency.
5. **Focus on Fine-Tuning Types**:
   - Instruction-Based: Task-specific improvements.
   - Continued Pre-Training: Domain adaptation with unlabeled data.
6. **Understand AWS-Specific Details**:
   - Data stored in **Amazon S3**.
   - Pricing tied to provisioned throughput.
7. **Time Management**:
   - Avoid spending too much time on metric formulasâ€”understand their purpose and application instead.


# Hands on:

- Model Evaluation
    - Build and evluation:
        - Automatic
        - Human : Bring your own work team.
        - Human : AWS Managed work team.
    - Create Evluation:
        - Automatic:
            - Evluation Name
            - DEscription
            - Model Select: 
                - Select base models
            - Task Type: 
                - General text generation
                - Text summarization.
                - Question and answer
                - Text classification
            - Metrics and datasets: For evaluation model performance
                - Metrics: Select
                    - Toxicity
                    - Accuracy
                    - Robustness
                - Choose a promt datasets
                    - Avalable build-in dataset
                        - REAL Toxicity
                        - Bold 
                    - Use your promt dataset from S3
            - Evaluation Results:
                - S3 location


